{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import tqdm\n",
    "\n",
    "from data.dataset_utils import get_dataset, get_transforms\n",
    "from detection_models.detection_model_utils import get_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Notebook\n",
    "\n",
    "## Load dataset\n",
    "\n",
    "We first load the dataset. We can either supply a path to a folder containing [`images`, `labels`] subfolders (e.g., `04_model_input` + `mode=\"val\" #\"train\"`) or to one of the raw `caseX` folders in `01_raw`, containing only images. In the latter case, the dataset will load the corresponding labels from `03_primary/labels` folder. \n",
    "\n",
    "Furthermore, we filter out positive or negative frames. The video sequences for the first 13 cases (the ones containing both positive and negative frames) are not \"continuous\", in the sense that positive and negative frames are not interleaved, but rather belong to separate clips/moments in the colonoscopy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative = False # whether to take negative or positive frames\n",
    "\n",
    "bbox_format = 'pascal_voc'\n",
    "resize = 512\n",
    "\n",
    "dataset = get_dataset(\n",
    "    '', \n",
    "    '/home/thuynh/data/01_raw/case63', \n",
    "    get_transforms('val', params={'transforms': {'resize': resize, 'min_area': 900, 'min_visibility': 0.25}, 'format': bbox_format}, normalize=True),\n",
    "    bbox_format\n",
    ")\n",
    "\n",
    "# only take the selected frames from the selected case and sort them sequentially\n",
    "if not negative:\n",
    "    dataset.images_list = sorted([x for x in dataset.images_list if 'Negative' not in x], key=lambda x: (int(x[-20:-18].strip('a')), int(x[-8:-4])))\n",
    "else: # select only the first 1200 negative frames for that case, to have a 20FPS video and have a representative sample\n",
    "    dataset.images_list = sorted([x for x in dataset.images_list if 'Negative' in x][:1200], key=lambda x: (int(x.split('Negative_')[-1][0]), int(x[-8:-4])) if x.split('Negative_')[-1][0].isdigit() else (0, int(x[-8:-4])))\n",
    "\n",
    "# Also update labels_list\n",
    "dataset.labels_list = [x[:-4]+'.json' for x in dataset.images_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model\n",
    "\n",
    "We load one trained model. We set `image_mean = (0,0,0)` and `image_std = (1,1,1)` because the `dataset.visualize()` method performs image normalization internally when provided with a model for inference.\n",
    "\n",
    "Instead, we set the confidence score threshold for accepting a prediction to `0.5`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset specifications\n",
    "ckpt_path = '/home/thuynh/torchvision_tutorial/runs/2022-05-11_18_56_13_2173/model_9.pth' \n",
    "\n",
    "model_name = 'fasterrcnn_mobilenet_v3_large_fpn'\n",
    "\n",
    "kwargs = {\"trainable_backbone_layers\": 5, \"min_size\": resize, \"max_size\": resize, 'image_mean': (0., 0., 0.), 'image_std': (1., 1., 1.), 'box_score_thresh': 0.5}\n",
    "\n",
    "num_classes = 7\n",
    "\n",
    "pretrained = True\n",
    "pretrained_backbone = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "if 'rcnn' in model_name:\n",
    "    model = torchvision.models.detection.__dict__[model_name](\n",
    "                pretrained=pretrained, pretrained_backbone=pretrained_backbone, **kwargs\n",
    "            )\n",
    "\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n",
    "else:\n",
    "    model = torchvision.models.detection.__dict__[model_name](\n",
    "                pretrained=pretrained, pretrained_backbone=pretrained_backbone, num_classes=num_classes, **kwargs\n",
    "            )\n",
    "\n",
    "# Load checkpoint\n",
    "ckpt = torch.load(ckpt_path, map_location='cpu')\n",
    "model.load_state_dict(ckpt['model'])\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference pass\n",
    "\n",
    "Next, we perform the inference pass using the `dataset.visualize()` method with the `model` argument specified. \n",
    "\n",
    "Internally, the `visualize` method loads all specified images (random ones otherwise) and corresponding labels. Then, performs the inference pass on the specified model and proceeds to plots the ground-truth and predicted (if any) bounding boxes onto the **original** image (n.b.: model input size and original image size do not correspond usually)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform inference + drawing\n",
    "images, grid = dataset.visualize(images_list=dataset.images_list, model=model, resize=resize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot in notebook (only for few images)\n",
    "# plt.figure(figsize=(10,40))\n",
    "# plt.axis('off')\n",
    "# plt.imshow(np.array(grid.permute(1,2,0)))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save sequence\n",
    "\n",
    "Finally, we save the inferenced sequences as a video using `opencv`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_no = dataset.images_path.split('/')[-2] # extract from filepath\n",
    "\n",
    "# Define FPS to make video fit within 60 seconds or at least 10FPS \n",
    "FPS = max(10.0, round(len(images)/60, 1))\n",
    "\n",
    "suffix = '_negative' if negative else '_positive'\n",
    "\n",
    "# Save video\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "\n",
    "height, width, channels = images[0].shape\n",
    "video_name = f'/home/thuynh/data/07_reporting/video/{model_name}/{case_no}{suffix}.avi'\n",
    "\n",
    "if not os.path.exists(os.path.split(video_name)[0]):\n",
    "    os.makedirs(os.path.split(video_name)[0])\n",
    "\n",
    "video_writer = cv2.VideoWriter(video_name, fourcc, fps=FPS, frameSize=(width, height))\n",
    "\n",
    "for img in tqdm.tqdm(images, total=len(images), desc=f'Creating video for {case_no}...'):\n",
    "    video_writer.write(cv2.cvtColor(img, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "video_writer.release()\n",
    "\n",
    "print(f'Video for \"{case_no}\" saved to: {video_name}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "82f8bbdca075e735cd7b5ecdbbc79d17d3ada3017f7b6c600a315b401a494d2b"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('ms-thesis': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "f2cf9091d097e7ecca4c42107bf4500547b5f744674b9516c53e9fc56a2d44a4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
