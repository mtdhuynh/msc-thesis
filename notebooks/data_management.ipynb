{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torchvision\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data & Annotations Management\n",
    "\n",
    "The following notebook is used to manage the [SUN Dataset](http://sundatabase.org/) data and annotations. \n",
    "\n",
    "## Requirements\n",
    "\n",
    "In order to use this, you need:\n",
    "* Positive cases bounding box annotations. These are contained in the `.txt` files in the [`01_raw/annotation_txt`](../data/01_raw/annotation_txt/) folder for each positive case.\n",
    "* Positive cases polyp pathological diagnosis. The above `.txt` files do not contain information regarding the specific polyp pathological diagnoses, which instead are contained in [`02_intermediate/positive_cases_description.txt`](../data/02_intermediate/positive_cases_description.txt).\n",
    "* An annotation template of your choice to save each **image**'s annotation (the `txt` files are one per positive case). We will use a custom template, somewhat similar to the COCO Annotation template. You can find it in the [`annotation_template.json` template](../data/02_intermediate/annotation_template.json).\n",
    "* A list of the possible polyp pathological diagnoses present in the SUN Dataset. Each polyp class will be associated with a specific ID and a color for visualization purposes. You can find it in the [`02_intermediate/polyp_classes.json` file](../data/02_intermediate/polyp_classes.json).\n",
    "* A list of the cases the negative frames in the SUN Dataset come from. You can find it in the [`02_intermediate/negative_cases_description.txt` file](../data/02_intermediate/negative_cases_description.txt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base working directory\n",
    "BASE_DIR = '/home/thuynh'\n",
    "\n",
    "# Data folders\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data\")\n",
    "RAW_DIR = os.path.join(DATA_DIR, \"01_raw\")\n",
    "INTERMEDIATE_DIR = os.path.join(DATA_DIR, \"02_intermediate\")\n",
    "PRIMARY_DIR = os.path.join(DATA_DIR, '03_primary')\n",
    "MODEL_INPUT_DIR = os.path.join(DATA_DIR, '04_model_input')\n",
    "\n",
    "# Required folders/files\n",
    "ANNOTATIONS_DIR = os.path.join(RAW_DIR, \"annotation_txt\") # Raw case-by-case annotations folder\n",
    "ANNOTATION_TEMPLATE = os.path.join(INTERMEDIATE_DIR, 'annotation_template.json') # Custom annotation template\n",
    "POLYP_CLASSES = os.path.join(INTERMEDIATE_DIR, 'polyp_classes.json') # SUN dataset polyp classes \n",
    "POSITIVE_CASES_DESCRIPTION = os.path.join(INTERMEDIATE_DIR, 'positive_cases_description.txt') # Polyp classes case-by-case\n",
    "NEGATIVE_CASES_DESCRIPTION = os.path.join(INTERMEDIATE_DIR, 'negative_cases_description.txt') # Negative frames and cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow\n",
    "\n",
    "In the `annotation_txt` folder we have the bounding box annotations for each case. However, these annotations are lacking the information on the specific polyp pathological diagnosis. We need to complement that information with the one coming from `positive_cases_description.txt` and `negative_cases_description`. For each case, we only have one polyp diagnosis. \n",
    "\n",
    "What we will do is:\n",
    "#### 1. Read each case annotation file (in `annotation_txt`) and save all cases' annotations in a `pd.DataFrame`.\n",
    "#### 2. Complement each case annotation with information on the specific polyp pathological diagnosis/class (from `positive_cases_description.txt`).\n",
    "#### 3. Add negative images information and class (from `negative_cases_description.txt`). \n",
    "#### 4. Save a separate annotation file for each image (following the `annotation_template.json` template structure) in `03_primary/labels` (negative images too).\n",
    "#### 5. Extract and move all images (positive and negative) to `03_primary/images`.\n",
    "#### 6. Split and save training and validation sets in `04_model_input/train` and `04_model_input/val`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Read bounding box annotations\n",
    "\n",
    "**N.B.**: cases #58 and #97 have multiple bbox coordinates for certain images. We selected only one out of the ones specified, based on the coordinates of the bounding box in the images before and after the images with multiple bbox coordinates. The original annotation files were saved as `_case58` and `_case97`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Filename', 'Min_X', 'Min_Y', 'Max_X', 'Max_Y', 'Frame_Class']\n",
    "\n",
    "# Extract list of annotations\n",
    "annotation_files = [f for f in os.listdir(ANNOTATIONS_DIR) if f.startswith('case')]\n",
    "annotation_files = sorted(annotation_files, key=lambda x: int(''.join(filter(str.isdigit, x)))) # sort list of files by ascending number\n",
    "\n",
    "# Store annotations case by case separately\n",
    "positive_annotations_dict = {}\n",
    "\n",
    "for idx, annotation in tqdm.tqdm(enumerate(annotation_files), desc='Loading annotations for each case...', total=len(annotation_files)):\n",
    "    with open(os.path.join(ANNOTATIONS_DIR, annotation), 'r') as f:\n",
    "        # Use a regex for the sep argument (filename is space-separated, coordinates are comma-separated)\n",
    "        # When using regex, need to specify the \"python\" engine\n",
    "        positive_annotations_dict[f'case{idx+1}'] = pd.read_csv(f, sep=' |,', names=columns, engine='python')\n",
    "        \n",
    "    # Combine bbox coordinates into a single column\n",
    "    positive_annotations_dict[f'case{idx+1}']['XYXY'] = positive_annotations_dict[f'case{idx+1}'].iloc[:, 1:5].to_numpy().tolist()\n",
    "\n",
    "    # Clean-up\n",
    "    positive_annotations_dict[f'case{idx+1}'].drop(columns=['Min_X', 'Min_Y', 'Max_X', 'Max_Y'], inplace=True)\n",
    "    positive_annotations_dict[f'case{idx+1}']['ID'] = idx+1\n",
    "\n",
    "# Concatenate everything in a single pd.DataFrame\n",
    "positive_annotations = pd.concat([case for case in positive_annotations_dict.values()]).reset_index(drop=True)\n",
    "\n",
    "# Boxes are in XYXY format, we also want XYWH format (aka COCO format) and CXCYWH format (denormalized YOLO format).\n",
    "positive_annotations['XYWH'] = positive_annotations['XYXY'].apply(lambda x: torchvision.ops.box_convert(torch.tensor(x), 'xyxy', 'xywh').tolist())\n",
    "positive_annotations['CXCYWH'] = positive_annotations['XYXY'].apply(lambda x: torchvision.ops.box_convert(torch.tensor(x), 'xyxy', 'cxcywh').tolist())\n",
    "\n",
    "\n",
    "positive_annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Complement annotations with specific polyp diagnoses.\n",
    "\n",
    "The polyp pathological diagnoses were extracted from the SUN Dataset homepage (_Table 2_), and saved in `positive_cases_description.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read positive cases description file\n",
    "with open(POSITIVE_CASES_DESCRIPTION, 'r') as f:\n",
    "    positive_cases_desc = pd.read_csv(f, sep='\\t', header=0, thousands=',').drop(columns=['Number of frames']).rename(columns={'Pathological diagnosis': 'Pathological_Diagnosis'})\n",
    "\n",
    "positive_cases_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the pathological diagnoses with the rest of the information\n",
    "positive_annotations = positive_annotations.merge(positive_cases_desc, on='ID', how='left')\n",
    "\n",
    "positive_annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Add also the negative frames information.\n",
    "\n",
    "As stated in [_Misawa et al._](https://www.giejournal.org/article/S0016-5107(20)34655-1/fulltext), a small number of negative images (1024 out of 56668) can help with the training and performances of the object detection models. Therefore, we add to our database also the negative frames and possibly experiment with different numbers of negative frames included in the training/validation sets. \n",
    "\n",
    "The information for the negative frames was extracted from the SUN Dataset homepage (_Table 3_), and saved in `negative_cases_description.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read negative cases description file\n",
    "with open(NEGATIVE_CASES_DESCRIPTION, 'r') as f:\n",
    "    negative_cases_desc = pd.read_csv(f, sep='\\t', header=0, thousands=',')\n",
    "\n",
    "negative_cases_desc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Negative frames (with no polyp in them) have `\"Negative\"` as part of their filename. We will use this information to filter them out from positive frames. Also, as seen in the above table, we only have negative frames for the first 13 cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_cases = defaultdict(list)\n",
    "# Retrieve negative images paths and cases\n",
    "negative_cases = {case_id: [image_path for image_path in glob.iglob('*.jpg', root_dir=os.path.join(RAW_DIR, f'case{case_id}')) if 'Negative' in image_path] for case_id in negative_cases_desc['ID']}\n",
    "\n",
    "# Drop cases with no negative frames\n",
    "negative_cases = {case_id: pd.DataFrame({'Filename': images, 'ID': case_id, 'Frame_Class': 1}) for case_id, images in negative_cases.items()} # Frame_Class = 1 means no polyp\n",
    "\n",
    "# Concatenate everything in a single pd.DataFrame\n",
    "negative_annotations = pd.concat([case for case in negative_cases.values()]).reset_index(drop=True)\n",
    "\n",
    "negative_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with the annotations dataframe from earlier\n",
    "annotations = positive_annotations.merge(negative_annotations, on=['ID', 'Filename', 'Frame_Class'], how='outer')\n",
    "\n",
    "# Clean-up\n",
    "annotations['XYXY'] = annotations['XYXY'].fillna(\"\").apply(list) # Use empty lists instead of NaNs\n",
    "annotations['XYWH'] = annotations['XYWH'].fillna(\"\").apply(list) # Use empty lists instead of NaNs\n",
    "annotations = annotations.groupby('ID').apply(lambda x: x.ffill().bfill()) # Fill information for negative frames with same from positive frames (they belong to the same case)\n",
    "\n",
    "annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above DataFrame contains all the information regarding the SUN Dataset: bounding box annotations (including polyp pathological diagnoses) for positive frames and negative frames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Save separate annotation files for each frame in `03_primary/labels`.\n",
    "\n",
    "The `03_primary` folder will be our main source of data from which we will sample out our training and validation sets to `04_model_input` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create \"labels\" dir if needed\n",
    "LABELS_DIR = os.path.join(PRIMARY_DIR, 'labels')\n",
    "LABELS_EXT = '.json'\n",
    "\n",
    "if not os.path.exists(LABELS_DIR):\n",
    "    os.mkdir(LABELS_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the custom annotation template we will be using for the SUN Dataset labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load \"annotation_template.json\"\n",
    "with open(ANNOTATION_TEMPLATE, 'r') as f:\n",
    "    annotation_template = json.load(f)\n",
    "\n",
    "annotation_template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each image, we will have an homonimous label `.json` file which will contain the following information:\n",
    "* `image_id`: name of the image.\n",
    "* `class`: either `0` or `1`, indicating a positive/polyp (`0`) or negative/non-polyp (`1`) frame.\n",
    "* `labels`: if `class=0`, then `labels` contains the ground-truth annotations for that frame. Annotations will include a unique identifier of the label(`label_id`), the ID of the pathological diagnosis (`category_id`. See `polyp_classes.json`), the segmentation mask polygonal coordinates - if any (`segmentation`), and the bounding box coordinates (`xyxy` and `xywh` for [x1, y1, x2, y2] and [x-center, y-center, bbox-width, bbox-height] formats, respectively).\n",
    "* `other`: any other additional note, comment, information regarding the particular image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now also load the pathological diagnosis classes present in the SUN Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load \"classes_template.json\"\n",
    "with open(POLYP_CLASSES, 'r') as f:\n",
    "    polyp_classes = json.load(f)\n",
    "\n",
    "polyp_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each polyp class, we assign a unique ID (`id`), the corresponding pathological diagnosis (`name`), and different colors for visualization purposes (`color` and `outline`. `color` should be used for the segmentation masks \"inside\", as the color is semi-transparent. `outline` should be used for the contour of segmentation masks or the bounding box itself)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can populate each annotation template for each image using the information from each annotation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**N.B.**: for the SUN Dataset we only have **one** bounding box annotation for each image. This might not be the case for images coming from other datasets or for possible future extensions of the SUN Dataset itself. As a matter of fact, there may be multiple objects detected in the same frame (be them other polyps or other objects of interests - such as image artefacts, etc.). \n",
    "\n",
    "Also, there might be multiple annotations associated with the same frame (bounding boxes, segmentation masks, etc.), therefore we keep our `labels` field as a **list of labels** (even though for the SUN Dataset we will only have one).\n",
    "\n",
    "This might come in handy in the future when extending the framework to other datasets or for more refined annotations for the SUN Dataset itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store all annotations in a single file for tidyness\n",
    "all_annotations = {'annotations': []}\n",
    "\n",
    "# Populate annotation template for each image\n",
    "# NB itertuples() returns a namedtuple for each row, whose fields/columns can be accessed as class attributes (via \"somenamedtuple.<fieldname>\")\n",
    "for image_row in tqdm.tqdm(annotations.itertuples(), desc=\"Populating and saving each image's annotations...\", total=len(annotations)):\n",
    "    # Copy template structure\n",
    "    image_annot = copy.deepcopy(annotation_template)\n",
    "    image_annot['labels'] = [] # Initialise empty\n",
    "    image_annot['other'] = []\n",
    "\n",
    "    # Populate\n",
    "    # Add image name and class\n",
    "    image_annot['image_id'] = image_row.Filename\n",
    "    image_annot['class'] = image_row.Frame_Class\n",
    "\n",
    "    # Add annotations\n",
    "    if image_row.Frame_Class == 0: # NB: Frame_Class=0 means POSITIVE (polyp) frame\n",
    "        # Extract polyp class ID\n",
    "        polyp_class = [polyp['id'] for polyp in polyp_classes['polyp_classes'] if polyp['name']==image_row.Pathological_Diagnosis]\n",
    "        \n",
    "        image_annot['labels'] = [\n",
    "            {\n",
    "                'label_id': image_row.Index+1, # Unique, sequential label ID\n",
    "                'category_id': polyp_class[0], # Polyp ID from classes_template.json\n",
    "                'category_name': image_row.Pathological_Diagnosis,\n",
    "                'xyxy': image_row.XYXY,\n",
    "                'xywh': image_row.XYWH,\n",
    "                'cxcywh': image_row.CXCYWH,\n",
    "                'bbox_width': image_row.XYWH[2],\n",
    "                'bbox_height': image_row.XYWH[3],\n",
    "                'bbox_ratio': round(image_row.XYWH[2] / image_row.XYWH[3], 2),\n",
    "                'bbox_area': int(image_row.XYWH[2] * image_row.XYWH[3]) # rectangle area w*h\n",
    "            }\n",
    "        ]\n",
    "\n",
    "    # Add other useful information\n",
    "    image_annot['other'] = [\n",
    "        {\n",
    "            'LabelID': image_row.Index+1,\n",
    "            'CaseID': image_row.ID,\n",
    "            'Polyp Shape': image_row.Shape,\n",
    "            'Polyp Size': image_row.Size,\n",
    "            'Polyp Location': image_row.Location\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    all_annotations['annotations'].append(image_annot)\n",
    "\n",
    "\n",
    "################################### RUN ONLY ONCE OR IF NEEDED ###################################\n",
    "# Save annotation file as \"filename.json\" in \"03_primary/labels\"\n",
    "    with open(os.path.join(LABELS_DIR, image_row.Filename[:-4]+LABELS_EXT), 'w') as f:\n",
    "        json.dump(image_annot, f)\n",
    "\n",
    "\n",
    "# Save annotations in a single json file located in \"03_primary/labels.json\"\n",
    "with open(os.path.join(PRIMARY_DIR, 'labels.json'), 'w') as f:\n",
    "    json.dump(all_annotations, f)\n",
    "\n",
    "###################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Move all images to `03_primary/images` folder.\n",
    "\n",
    "Finally, we copy all of the images (both positive AND negative frames) to the above-mentioned folder, from which we will sample out the training/validation sets (just like for the labels). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGES_DIR = os.path.join(PRIMARY_DIR, 'images')\n",
    "\n",
    "# Make sure the folder exists\n",
    "if not os.path.exists(IMAGES_DIR):\n",
    "    os.mkdir(IMAGES_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################### RUN ONLY ONCE OR IF NEEDED ###################################\n",
    "# Copy images to selected folder\n",
    "for image_path in tqdm.tqdm(glob.iglob(os.path.join(RAW_DIR, 'case*/*.jpg')), desc='Copying images from raw to primary folder...', total=len(annotations)):\n",
    "    # Get the image name only\n",
    "    image_name = os.path.basename(image_path)\n",
    "\n",
    "    # Copy image\n",
    "    shutil.copyfile(image_path, os.path.join(IMAGES_DIR, image_name))\n",
    "###################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Split and save training and validation sets in `04_model_input/train` and in `04_model_input/val`.\n",
    "\n",
    "For storage issues purposes, we will use `os.symlink` to just create shortcuts to the files in the `03_primary` folder. \n",
    "\n",
    "First, we need to set the **RNG seed** for reproducible results. `np.random.seed()` is not the recommended way to fix the seed anymore. Instead, we use `np.random.default_rng()`. See the [documentation](https://numpy.org/doc/stable/reference/random/generator.html?highlight=default_rng#) and [this blogpost](https://towardsdatascience.com/stop-using-numpy-random-seed-581a9972805f) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix seed for reproducibility\n",
    "SEED = 42\n",
    "RNG = np.random.default_rng(seed=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will only split into training and validation sets to exploit the SUN Dataset the most. For testing purposes, we can use any of the publicly available colonoscopy datasets - which would also provide generalizabity tests for our models.\n",
    "\n",
    "We perform train/val split on a 80/20 and **per-class** basis. \n",
    "\n",
    "A list of public datasets for automatic polyp detection can be found [here](https://github.com/sing-group/deep-learning-colonoscopy#public-datasets).\n",
    "\n",
    "Also, we will not include all negative frames in our dataset, but only some of them (as detailed in [_Misawa et al._](https://www.giejournal.org/article/S0016-5107(20)34655-1/fulltext)). However, we will use the number of negative frames as a hyperparameter and evaluate our models against varying number of negative samples (therefore, we will implement a method in our dataset to only take `N` negative frames).\n",
    "\n",
    "For now, we include all of them in the training and validation sets (but we make sure to include an even split of positive and negative frames)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get negative frames list\n",
    "negative_list = negative_annotations['Filename'].to_list()\n",
    "\n",
    "# Shuffle it (in-place)\n",
    "RNG.shuffle(negative_list)\n",
    "\n",
    "# Only take 5% of the negative images (~5k negative frames -- SUN dataset paper used ~1k)\n",
    "NEG_RATIO = 0.05\n",
    "negative_list = RNG.choice(negative_list, int(NEG_RATIO * len(negative_list)), replace=False)\n",
    "\n",
    "# Get training and validation split indices\n",
    "SPLIT = 0.8\n",
    "NEG_SPLIT = int(SPLIT * len(negative_list))\n",
    "\n",
    "# For each positive annotation, get each polyp class filelist (shuffled) and the split\n",
    "classes_list = {}\n",
    "classes_split = {}\n",
    "\n",
    "for polyp in polyp_classes['polyp_classes']:\n",
    "    classes_list[polyp['name']] = positive_annotations.loc[positive_annotations['Pathological_Diagnosis']==polyp['name'], 'Filename'].to_list()\n",
    "    RNG.shuffle(classes_list[polyp['name']]) # shuffle in place\n",
    "    # Get split indices\n",
    "    classes_split[polyp['name']] = int(SPLIT * len(classes_list[polyp['name']]))\n",
    "\n",
    "# Save neatly in a dict\n",
    "dataset = {}\n",
    "\n",
    "dataset['train'] = {}\n",
    "dataset['val'] = {}\n",
    "\n",
    "# Positive train/val split\n",
    "train_positive = [polyp_list[:split] for polyp_list, split in zip(classes_list.values(), classes_split.values())] # list of lists\n",
    "val_positive = [polyp_list[split:] for polyp_list, split in zip(classes_list.values(), classes_split.values())] # list of lists\n",
    "# Unpack\n",
    "train_positive = [fname for sublist in train_positive for fname in sublist]\n",
    "val_positive = [fname for sublist in val_positive for fname in sublist]\n",
    "\n",
    "# Extract positive images and labels\n",
    "dataset['train']['positive'] = {\n",
    "    'images': train_positive,\n",
    "    'labels': [fname[:-4]+LABELS_EXT for fname in train_positive]\n",
    "}\n",
    "dataset['val']['positive'] = {\n",
    "    'images': val_positive,\n",
    "    'labels': [fname[:-4]+LABELS_EXT for fname in val_positive]\n",
    "}\n",
    "\n",
    "# Negative train/val split\n",
    "train_negative = negative_list[:NEG_SPLIT]\n",
    "val_negative = negative_list[NEG_SPLIT:]\n",
    "\n",
    "# Extract negative images and labels\n",
    "dataset['train']['negative'] = {\n",
    "    'images': train_negative,\n",
    "    'labels': [fname[:-4]+LABELS_EXT for fname in train_negative]\n",
    "}\n",
    "dataset['val']['negative'] = {\n",
    "    'images': val_negative,\n",
    "    'labels': [fname[:-4]+LABELS_EXT for fname in val_negative]\n",
    "}\n",
    "\n",
    "# Sanity check\n",
    "print('---POSITIVE FRAMES---')\n",
    "print(f\"Trainset: {len(dataset['train']['positive']['images'])} | Valset: {len(dataset['val']['positive']['images'])} | Total: {len(dataset['train']['positive']['images']) + len(dataset['val']['positive']['images'])}\")\n",
    "\n",
    "print('---NEGATIVE FRAMES---')\n",
    "print(f\"Trainset: {len(dataset['train']['negative']['images'])} | Valset: {len(dataset['val']['negative']['images'])} | Total: {len(dataset['train']['negative']['images']) + len(dataset['val']['negative']['images'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASPLITS = ('train', 'val')\n",
    "FRAME_TYPES = ('positive', 'negative')\n",
    "XY = ('images', 'labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folders\n",
    "for datasplit in DATASPLITS:\n",
    "    INPUT_DIR = os.path.join(MODEL_INPUT_DIR, datasplit)\n",
    "    # Check if \"train\" or \"val\" folders already exist\n",
    "    if os.path.exists(INPUT_DIR):\n",
    "        print(f'Removing existing {datasplit} folder...')\n",
    "        shutil.rmtree(INPUT_DIR)\n",
    "    print(f'Created {datasplit} folder.')\n",
    "    os.mkdir(INPUT_DIR)\n",
    "    \n",
    "    # Create also images and labels folders\n",
    "    for xy in XY:\n",
    "        XY_DIR = os.path.join(INPUT_DIR, xy)\n",
    "        # Check if \"images\" or \"labels\" folders already exist\n",
    "        if not os.path.exists(XY_DIR):\n",
    "            print(f'Created {datasplit}/{xy} folder.')\n",
    "            os.mkdir(XY_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################### RUN ONLY ONCE OR IF NEEDED ###################################\n",
    "# Create symlink of the corresponding image and label in \"04_model_input/*/images\" and \"04_model_input/*/labels\"\n",
    "for datasplit in DATASPLITS:\n",
    "    for frame_type in FRAME_TYPES:\n",
    "        for xy in XY:\n",
    "            # Pair filepaths in a dict {src: dst}\n",
    "            filepairs = {os.path.join(PRIMARY_DIR, xy, fname): os.path.join(MODEL_INPUT_DIR, datasplit, xy, fname) for fname in dataset[datasplit][frame_type][xy]}\n",
    "\n",
    "            for src, dst in tqdm.tqdm(filepairs.items(), desc=f'Creating symlinks for {datasplit} set for {frame_type} {xy}...', total=len(dataset[datasplit][frame_type][xy])):\n",
    "                os.symlink(src, dst)\n",
    "###################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "for datasplit in DATASPLITS:\n",
    "    x = [img[:-4] for img in os.listdir(os.path.join(MODEL_INPUT_DIR, datasplit, 'images'))]\n",
    "    y = [lbl[:-5] for lbl in os.listdir(os.path.join(MODEL_INPUT_DIR, datasplit, 'labels'))]\n",
    "\n",
    "    assert sorted(x) == sorted(y), f'There are some mismatches or missing files between images and labels for the {datasplit} set.'   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "82f8bbdca075e735cd7b5ecdbbc79d17d3ada3017f7b6c600a315b401a494d2b"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('ms-thesis': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
